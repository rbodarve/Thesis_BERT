{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbc934dd",
   "metadata": {},
   "source": [
    "DistilXLM-RoBERTa for NSFW word detection and export to ONNX/TFLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a0f0674d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from datasets import Dataset\n",
    "import tempfile\n",
    "import subprocess\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bfae11f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if sys.platform.startswith('win'):\n",
    "    import locale\n",
    "    locale.setlocale(locale.LC_ALL, 'en_US.UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "64de0051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a8b732",
   "metadata": {},
   "source": [
    "IMPORT ADDITIONAL SETUP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f9808623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "# from optimum.onnxruntime.configuration import OptimizationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3601e9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_dataset():\n",
    "    sample_data = {\n",
    "        'text': [\n",
    "            # Safe words/phrases\n",
    "            'hello', 'world', 'computer', 'programming', 'science', 'education',\n",
    "            'family', 'friendship', 'learning', 'knowledge', 'book', 'music',\n",
    "            'art', 'nature', 'technology', 'innovation', 'creativity', 'peace',\n",
    "            'happiness', 'success', 'achievement', 'progress', 'development',\n",
    "            'community', 'cooperation', 'collaboration', 'respect', 'kindness',\n",
    "            # NSFW words (examples - replace with actual dataset)\n",
    "            'yawa', 'gago', 'shit', 'pakyu',\n",
    "            'puta', 'pisti', 'puki', 'buang',\n",
    "            'motherfucker', 'bitch', 'cunt', 'ulol',\n",
    "            'fuck', 'gaga', 'tanga', 'dick',\n",
    "            'fucker', 'putangina', 'bobo', 'asshole'\n",
    "        ],\n",
    "        'labels': [0] * 28 + [1] * 20  # 28 safe, 20 nsfw\n",
    "    }\n",
    "    \n",
    "    df = pd.DataFrame(sample_data)\n",
    "    df.to_csv('dataset.csv', index=False)\n",
    "    print(\"Sample dataset created and saved as 'dataset.csv'\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2c7347b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(csv_path='dataset.csv'):\n",
    "    print(f\"Loading dataset from {csv_path}...\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(csv_path)\n",
    "        print(f\"Dataset loaded successfully. Shape: {df.shape}\")\n",
    "        \n",
    "        required_cols = ['text', 'labels']\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing required columns: {missing_cols}\")\n",
    "        \n",
    "        df = df.dropna(subset=['text', 'labels'])\n",
    "        df['text'] = df['text'].astype(str)\n",
    "        df['labels'] = df['labels'].astype(int)\n",
    "        \n",
    "        unique_labels = df['labels'].unique()\n",
    "        if not all(labels in [0, 1] for labels in unique_labels):\n",
    "            raise ValueError(\"Labels must be 0 (safe) or 1 (nsfw)\")\n",
    "        \n",
    "        print(f\"Dataset validation complete. Clean shape: {df.shape}\")\n",
    "        print(f\"Labels distribution:\\n{df['labels'].value_counts()}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Dataset file '{csv_path}' not found!\")\n",
    "        print(\"Creating a sample dataset for demonstration...\")\n",
    "        return create_sample_dataset()\n",
    "    \n",
    "def split_dataset(df, test_size=0.2, random_state=42):\n",
    "    print(f\"Splitting dataset: {test_size*100}% for validation...\")\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        df['text'].tolist(),\n",
    "        df['labels'].tolist(),\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=df['labels']\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {len(X_train)} samples\")\n",
    "    print(f\"Validation set: {len(X_val)} samples\")\n",
    "    \n",
    "    return X_train, X_val, y_train, y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de1aea3",
   "metadata": {},
   "source": [
    "TOKENIZATION AND PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d5bd73d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tokenized_datasets(X_train, X_val, y_train, y_val, model_name):\n",
    "    print(\"Loading tokenizer and creating tokenized datasets...\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    train_dataset = Dataset.from_dict({\n",
    "        'text': X_train,\n",
    "        'labels': y_train\n",
    "    })\n",
    "    \n",
    "    val_dataset = Dataset.from_dict({\n",
    "        'text': X_val,\n",
    "        'labels': y_val\n",
    "    })\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples['text'],\n",
    "            truncation=True,\n",
    "            padding=False,  \n",
    "            max_length=128  \n",
    "        )\n",
    "    \n",
    "    train_tokenized = train_dataset.map(tokenize_function, batched=True)\n",
    "    val_tokenized = val_dataset.map(tokenize_function, batched=True)\n",
    "    \n",
    "    print(\"Tokenization complete!\")\n",
    "    return train_tokenized, val_tokenized, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12cf0f8",
   "metadata": {},
   "source": [
    "MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "76d0213a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_dataset, val_dataset, tokenizer, model_name, output_dir):\n",
    "    print(\"Initializing model for training...\")\n",
    "    \n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=2,\n",
    "        id2label={0: \"SAFE\", 1: \"NSFW\"},\n",
    "        label2id={\"SAFE\": 0, \"NSFW\": 1},\n",
    "    )\n",
    "    \n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        learning_rate=2e-5,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=f'{output_dir}/logs',\n",
    "        logging_steps=10,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"accuracy\",\n",
    "        greater_is_better=True,\n",
    "        report_to=[],  \n",
    "        seed=42,\n",
    "        dataloader_num_workers=0,  \n",
    "        remove_unused_columns=True\n",
    "    )\n",
    "    \n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        predictions = np.argmax(predictions, axis=1)\n",
    "        \n",
    "        return {\n",
    "            'accuracy': accuracy_score(labels, predictions),\n",
    "        }\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    trainer.train()\n",
    "    \n",
    "    print(f\"Saving model to {output_dir}...\")\n",
    "    trainer.save_model()\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    return trainer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce68ecd",
   "metadata": {},
   "source": [
    "MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "88ed7243",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(trainer, y_val, output_dir):\n",
    "    print(\"Evaluating model performance...\")\n",
    "\n",
    "    eval_results = trainer.evaluate()\n",
    "    \n",
    "    predictions = trainer.predict(trainer.eval_dataset)\n",
    "    y_pred = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "    from sklearn.metrics import classification_report, accuracy_score\n",
    "    \n",
    "    report = classification_report(\n",
    "        y_val, y_pred, target_names=[\"SAFE\", \"NSFW\"], digits=4\n",
    "    )\n",
    "\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "\n",
    "    metrics_text = f\"\"\"NSFW Detection Model Evaluation Results\n",
    "    {'='*50}\n",
    "\n",
    "    Accuracy: {accuracy:.4f}\n",
    "\n",
    "    Classification Report:\n",
    "    {report}\n",
    "\n",
    "    Training Results:\n",
    "    {'-'*30}\n",
    "    \"\"\"\n",
    "\n",
    "    for key, value in eval_results.items():\n",
    "        metrics_text += f\"{key}: {value:.4f}\\n\"\n",
    "\n",
    "    os.makedirs(\"metrics\", exist_ok=True)\n",
    "    metrics_path = \"metrics/metrics.txt\"\n",
    "\n",
    "    with open(metrics_path, \"w\", encoding='utf-8') as f:  \n",
    "        f.write(metrics_text)\n",
    "\n",
    "    print(f\"Metrics saved to {metrics_path}\")\n",
    "    print(f\"Validation Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    return accuracy, report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a462e5e1",
   "metadata": {},
   "source": [
    "ONNX EXPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d9d79f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def export_to_onnx(model_dir, onnx_path):\n",
    "#     print(\"Exporting model to ONNX format...\")\n",
    "\n",
    "#     try:\n",
    "#         from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "#         from optimum.exporters.onnx import main_export\n",
    "#         from pathlib import Path\n",
    "        \n",
    "#         onnx_dir = os.path.dirname(onnx_path)\n",
    "#         os.makedirs(onnx_dir, exist_ok=True)\n",
    "\n",
    "#         try:\n",
    "#             onnx_model = ORTModelForSequenceClassification.from_pretrained(\n",
    "#                 model_dir, \n",
    "#                 export=True,\n",
    "#                 use_cache=False  \n",
    "#             )\n",
    "#             onnx_model.save_pretrained(onnx_dir)\n",
    "            \n",
    "#             onnx_files = list(Path(onnx_dir).glob(\"*.onnx\"))\n",
    "#             if onnx_files:\n",
    "#                 current_onnx = onnx_files[0]\n",
    "#                 target_path = Path(onnx_path)\n",
    "#                 if current_onnx != target_path:\n",
    "#                     if target_path.exists():\n",
    "#                         target_path.unlink()\n",
    "#                     current_onnx.rename(target_path)\n",
    "                \n",
    "#                 print(f\"ONNX model exported to: {onnx_path}\")\n",
    "#                 return True\n",
    "                \n",
    "#         except Exception as e1:\n",
    "#             print(f\"Method 1 failed: {e1}, trying alternative...\")\n",
    "            \n",
    "#             from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "#             import torch\n",
    "            \n",
    "#             model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "#             tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "            \n",
    "#             dummy_text = \"sample input text\"\n",
    "#             dummy_input = tokenizer(\n",
    "#                 dummy_text,\n",
    "#                 return_tensors=\"pt\",\n",
    "#                 max_length=128,\n",
    "#                 padding=\"max_length\",\n",
    "#                 truncation=True,\n",
    "#             )\n",
    "            \n",
    "#             torch.onnx.export(\n",
    "#                 model,\n",
    "#                 (dummy_input['input_ids'], dummy_input['attention_mask']),\n",
    "#                 onnx_path,\n",
    "#                 export_params=True,\n",
    "#                 opset_version=14,  # Compatible with your onnxruntime 1.19.0\n",
    "#                 do_constant_folding=True,\n",
    "#                 input_names=['input_ids', 'attention_mask'],\n",
    "#                 output_names=['logits'],\n",
    "#                 dynamic_axes={\n",
    "#                     'input_ids': {0: 'batch_size', 1: 'sequence'},\n",
    "#                     'attention_mask': {0: 'batch_size', 1: 'sequence'},\n",
    "#                     'logits': {0: 'batch_size'}\n",
    "#                 },\n",
    "#                 verbose=True\n",
    "#             )\n",
    "            \n",
    "#             print(f\"ONNX model exported to: {onnx_path}\")\n",
    "#             return True\n",
    "\n",
    "#     except ImportError as e:\n",
    "#         print(f\"Import error: {e}\")\n",
    "#         return False\n",
    "#     except Exception as e:\n",
    "#         print(f\"ONNX export failed: {e}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "#         return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e05eab",
   "metadata": {},
   "source": [
    "TENSORFLOW LITE EXPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6f4c4fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_tflite_alternative(model_dir, tflite_path):\n",
    "    try:\n",
    "        import tensorflow as tf\n",
    "        from transformers import TFAutoModelForSequenceClassification, AutoTokenizer\n",
    "        \n",
    "        print(f\"Using TensorFlow {tf.__version__}\")\n",
    "        \n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "        \n",
    "        tf_model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "            model_dir, \n",
    "            from_pt=True  # Correct parameter for transformers 4.44.2\n",
    "        )\n",
    "\n",
    "        converter = tf.lite.TFLiteConverter.from_keras_model(tf_model)\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        \n",
    "        def representative_dataset():\n",
    "            sample_texts = [\n",
    "                \"hello world\", \"test message\", \"sample text\", \"another example\",\n",
    "                \"short\", \"this is a longer text for testing purposes\"\n",
    "            ]\n",
    "            \n",
    "            for text in sample_texts:\n",
    "                inputs = tokenizer(\n",
    "                    text, \n",
    "                    return_tensors=\"tf\", \n",
    "                    max_length=128, \n",
    "                    padding=\"max_length\",\n",
    "                    truncation=True\n",
    "                )\n",
    "                if 'token_type_ids' in inputs:\n",
    "                    yield [inputs['input_ids'], inputs['attention_mask'], inputs['token_type_ids']]\n",
    "                else:\n",
    "                    yield [inputs['input_ids'], inputs['attention_mask']]\n",
    "                # yield [inputs['input_ids'], inputs['attention_mask']]\n",
    "        \n",
    "        converter.representative_dataset = representative_dataset\n",
    "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS]\n",
    "        \n",
    "        tflite_model = converter.convert()\n",
    "\n",
    "        os.makedirs(os.path.dirname(tflite_path), exist_ok=True)\n",
    "        with open(tflite_path, \"wb\") as f:\n",
    "            f.write(tflite_model)\n",
    "\n",
    "        print(f\"TFLite model exported to: {tflite_path}\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"TFLite export failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "44831ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def export_to_tflite(model_dir, tflite_path):\n",
    "#     print(\"Converting model to TensorFlow Lite...\")\n",
    "\n",
    "#     try:\n",
    "#         success = export_tflite_alternative(model_dir, tflite_path)\n",
    "#         if success:\n",
    "#             return True\n",
    "            \n",
    "#         print(\"Trying ONNX -> TensorFlow -> TFLite conversion...\")\n",
    "        \n",
    "#         # import onnx\n",
    "#         # import tensorflow as tf\n",
    "#         # from onnx_tf.backend import prepare\n",
    "#         import tempfile\n",
    "        \n",
    "#         temp_onnx = os.path.join(tempfile.gettempdir(), \"temp_model.onnx\")\n",
    "        \n",
    "#         from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "#         import torch\n",
    "        \n",
    "#         model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "#         tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "        \n",
    "#         dummy_input = tokenizer(\n",
    "#             \"sample text\",\n",
    "#             return_tensors=\"pt\",\n",
    "#             max_length=128,\n",
    "#             padding=\"max_length\",\n",
    "#             truncation=True,\n",
    "#         )\n",
    "        \n",
    "#         torch.onnx.export(\n",
    "#             model,\n",
    "#             (dummy_input['input_ids'], dummy_input['attention_mask']),\n",
    "#             temp_onnx,\n",
    "#             export_params=True,\n",
    "#             opset_version=12,  # Lower opset for better onnx-tf compatibility\n",
    "#             do_constant_folding=True,\n",
    "#             input_names=['input_ids', 'attention_mask'],\n",
    "#             output_names=['logits']\n",
    "#         )\n",
    "        \n",
    "#         # onnx_model = onnx.load(temp_onnx)\n",
    "#         # tf_rep = prepare(onnx_model)\n",
    "        \n",
    "#         with tempfile.TemporaryDirectory() as temp_dir:\n",
    "#             saved_model_path = os.path.join(temp_dir, \"saved_model\")\n",
    "#             tf_rep.export_graph(saved_model_path)\n",
    "            \n",
    "#             converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\n",
    "#             converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "            \n",
    "#             tflite_model = converter.convert()\n",
    "            \n",
    "#             os.makedirs(os.path.dirname(tflite_path), exist_ok=True)\n",
    "#             with open(tflite_path, \"wb\") as f:\n",
    "#                 f.write(tflite_model)\n",
    "            \n",
    "#         if os.path.exists(temp_onnx):\n",
    "#             os.remove(temp_onnx)\n",
    "            \n",
    "#         print(f\"TFLite model exported to: {tflite_path}\")\n",
    "#         return True\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"TFLite export failed: {e}\")\n",
    "#         import traceback\n",
    "#         traceback.print_exc()\n",
    "#         return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e1eec33",
   "metadata": {},
   "source": [
    "MAIN TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "514a94bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Starting NSFW Detection Model Training Pipeline\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    MODEL_NAME = \"microsoft/xtremedistil-l6-h256-uncased\"\n",
    "    OUTPUT_DIR = \"models/exporter-xtremedistil-l6-h256-uncased\"\n",
    "    # ONNX_PATH = \"models/exporter_nsfw_model.onnx\"\n",
    "    TFLITE_PATH = \"models/exporter_nsfw_model.tflite\"\n",
    "\n",
    "    os.makedirs(\"models\", exist_ok=True)\n",
    "    os.makedirs(\"metrics\", exist_ok=True)\n",
    "\n",
    "    df = load_dataset()\n",
    "\n",
    "    X_train, X_val, y_train, y_val = split_dataset(df)\n",
    "\n",
    "    train_dataset, val_dataset, tokenizer = create_tokenized_datasets(\n",
    "        X_train, X_val, y_train, y_val, MODEL_NAME\n",
    "    )\n",
    "\n",
    "    trainer, model = train_model(\n",
    "        train_dataset, val_dataset, tokenizer, MODEL_NAME, OUTPUT_DIR\n",
    "    )\n",
    "\n",
    "    accuracy, report = evaluate_model(trainer, y_val, OUTPUT_DIR)\n",
    "\n",
    "    # onnx_success = export_to_onnx(OUTPUT_DIR, ONNX_PATH)\n",
    "\n",
    "    # tflite_success = False\n",
    "    # if onnx_success:\n",
    "    #     tflite_success = export_to_tflite(OUTPUT_DIR, TFLITE_PATH)\n",
    "    # else:\n",
    "    tflite_success = export_tflite_alternative(OUTPUT_DIR, TFLITE_PATH)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Training and Export Summary:\")\n",
    "    print(f\" Model trained successfully - Accuracy: {accuracy:.4f}\")\n",
    "    # print(f\"{'' if onnx_success else ''} ONNX export: {'SUCCESS' if onnx_success else 'FAILED'}\")\n",
    "    print(f\"{'' if tflite_success else ''} TFLite export: {'SUCCESS' if tflite_success else 'FAILED'}\")\n",
    "    print(f\"\\nOutput files:\")\n",
    "    print(f\"  - Model: {OUTPUT_DIR}\")\n",
    "    print(f\"  - Metrics: metrics/metrics.txt\")\n",
    "    # if onnx_success:\n",
    "    #     print(f\"  - ONNX: {ONNX_PATH}\")\n",
    "    if tflite_success:\n",
    "        print(f\"  - TFLite: {TFLITE_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f77849",
   "metadata": {},
   "source": [
    "Sample CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "abf20769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference(model_dir, test_texts=None):\n",
    "    if test_texts is None:\n",
    "        test_texts = [\"hello world\", \"putang ina\"]\n",
    "    \n",
    "    print(\"\\nTesting trained model...\")\n",
    "    \n",
    "    try:\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(model_dir)\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        for text in test_texts:\n",
    "            inputs = tokenizer(\n",
    "                text, \n",
    "                return_tensors=\"pt\", \n",
    "                truncation=True, \n",
    "                max_length=128,\n",
    "                padding=True\n",
    "            )\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "                predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "                predicted_class = torch.argmax(predictions, dim=-1).item()\n",
    "                confidence = predictions[0][predicted_class].item()\n",
    "            \n",
    "            labels = \"SAFE\" if predicted_class == 0 else \"NSFW\"\n",
    "            print(f\"Text: '{text}' -> {labels} (confidence: {confidence:.4f})\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Inference test failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4544fa7b",
   "metadata": {},
   "source": [
    "PROGRAM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0aeff701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting NSFW Detection Model Training Pipeline\n",
      "============================================================\n",
      "Loading dataset from dataset.csv...\n",
      "Dataset loaded successfully. Shape: (48, 2)\n",
      "Dataset validation complete. Clean shape: (48, 2)\n",
      "Labels distribution:\n",
      "labels\n",
      "0    28\n",
      "1    20\n",
      "Name: count, dtype: int64\n",
      "Splitting dataset: 20.0% for validation...\n",
      "Training set: 38 samples\n",
      "Validation set: 10 samples\n",
      "Loading tokenizer and creating tokenized datasets...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 38/38 [00:00<00:00, 5115.83 examples/s]\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 2606.78 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenization complete!\n",
      "Initializing model for training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/xtremedistil-l6-h256-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_101265/662042128.py:42: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [15/15 00:01, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.689414</td>\n",
       "      <td>0.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.686026</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.682627</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.688500</td>\n",
       "      <td>0.680047</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.688500</td>\n",
       "      <td>0.678970</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu-rainn/Projects/Thesis_BERT/bertenv/lib/python3.12/site-packages/transformers/trainer.py:4360: UserWarning: mtime may not be reliable on this filesystem, falling back to numerical ordering\n",
      "  warnings.warn(\"mtime may not be reliable on this filesystem, falling back to numerical ordering\")\n",
      "/home/ubuntu-rainn/Projects/Thesis_BERT/bertenv/lib/python3.12/site-packages/transformers/trainer.py:4360: UserWarning: mtime may not be reliable on this filesystem, falling back to numerical ordering\n",
      "  warnings.warn(\"mtime may not be reliable on this filesystem, falling back to numerical ordering\")\n",
      "/home/ubuntu-rainn/Projects/Thesis_BERT/bertenv/lib/python3.12/site-packages/transformers/trainer.py:4360: UserWarning: mtime may not be reliable on this filesystem, falling back to numerical ordering\n",
      "  warnings.warn(\"mtime may not be reliable on this filesystem, falling back to numerical ordering\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to models/exporter-xtremedistil-l6-h256-uncased...\n",
      "Evaluating model performance...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics saved to metrics/metrics.txt\n",
      "Validation Accuracy: 1.0000\n",
      "Using TensorFlow 2.20.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpp07_w5xx/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpp07_w5xx/assets\n",
      "/home/ubuntu-rainn/Projects/Thesis_BERT/bertenv/lib/python3.12/site-packages/tensorflow/lite/python/convert.py:863: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  \n",
      "W0000 00:00:1757003892.370870  101265 tf_tfl_flatbuffer_helpers.cc:364] Ignored output_format.\n",
      "W0000 00:00:1757003892.370899  101265 tf_tfl_flatbuffer_helpers.cc:367] Ignored drop_control_dependency.\n",
      "2025-09-05 00:38:12.371093: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpp07_w5xx\n",
      "2025-09-05 00:38:12.387072: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-09-05 00:38:12.387099: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpp07_w5xx\n",
      "2025-09-05 00:38:12.502565: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-09-05 00:38:12.771321: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpp07_w5xx\n",
      "2025-09-05 00:38:12.917062: I tensorflow/cc/saved_model/loader.cc:471] SavedModel load for tags { serve }; Status: success: OK. Took 545968 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: FLOAT32, output_inference_type: FLOAT32\n",
      "2025-09-05 00:38:17.815904: W tensorflow/compiler/mlir/lite/flatbuffer_export.cc:3705] Skipping runtime version metadata in the model. This will be generated by the exporter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite model exported to: models/exporter_nsfw_model.tflite\n",
      "\n",
      "============================================================\n",
      "Training and Export Summary:\n",
      " Model trained successfully - Accuracy: 1.0000\n",
      " TFLite export: SUCCESS\n",
      "\n",
      "Output files:\n",
      "  - Model: models/exporter-xtremedistil-l6-h256-uncased\n",
      "  - Metrics: metrics/metrics.txt\n",
      "  - TFLite: models/exporter_nsfw_model.tflite\n",
      "\n",
      "Testing trained model...\n",
      "Text: 'hello world' -> SAFE (confidence: 0.5023)\n",
      "Text: 'putang ina' -> NSFW (confidence: 0.5048)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "        \n",
    "        test_inference(\"models/exporter-xtremedistil-l6-h256-uncased\")\n",
    "        \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nTraining interrupted by user.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error during execution: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "09fbc810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.56.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "46cd8d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('4.56.0', '1.10.1')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import accelerate\n",
    "import transformers\n",
    "\n",
    "transformers.__version__, accelerate.__version__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bertenv (3.12.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
